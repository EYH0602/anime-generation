\subsection*{Background}

\subsubsection*{GAN}
Generative Adversarial Networks train a generator $G$ and a discriminator $D$ together through a mini-max game.
The generator takes in random noise and maps the noise to an image.
The discriminator takes in an image and tells if it is real from the dataset or generated by the generator.
To train these two networks simultaneously, Goodfellow et al. defined the loss function as 
$$
\min_G\max_{D}\mathcal{L}(D, G) = 
\mathbb{E}_{x \sim p_{data}}[\log D(x)] + 
\mathbb{E}_{z \sim p_z}[\log (1 - D(G(z)))]
$$
where $p_{data}$ is the images in the real dataset
and $p_z$ is randomly generated vectors in the same shape as the real images \cite{Goodfellow2020Generative}.

\par
Based on the above models, Karras et al. proposed StyleGAN as 
a GAN variant that can separate high-level feature variations and scale-specific control these variations \cite{Karras2019Style}.
If trained on a dataset of human faces, these high-level features include eyes, nose, hair, etc.
Together with GAN's two-model nature, mapping the high-level features of anime characters to real human pictures becomes possible.
Min Jin Chong and Davis Forsyth proposed two GAN-based models,
GANs N' Roses (GNR) and JoJoGAN,
to map anime image style to human pictures \cite{chong2021gans,chong2021jojogan}.
GNR trained on different anime characters will learn different styles,
thus producing a sequence of diverse outputs based on the same input image.
\begin{figure}[h]
    \includegraphics[width=\textwidth]{img/GNR-1.png}
    \caption{
        GANs N' Roses: The first column on the left is the inputed human picture.
        Each following column shows a different style learned from different anime mapped to that human \cite{chong2021gans}.
    }
\end{figure}

JoJoGAN, however, takes in an anime character's image as a reference and generates
a style mapper that can be applied to human faces.
The key difference between JoJoGAN and GNR is that GNR needs a dataset of the same anime character to learn the style of that character.
Instead of finding a large number of quality images of the same character,
JoJoGAN can be trained on a generalized anime dataset,
then produce the needed style map based on the given single reference.
This difference allows JoJoGAN to generate multiple images of multiple much easier than other style-based image generators.
\begin{figure}[h]
    \includegraphics[width=\textwidth]{img/JoJo-1.png}
    \caption{
        JoJoGAN: The first column on the left is the input human picture.
        Each following column shows a different style generated from different reference anime images \cite{chong2021jojogan}.
    }
\end{figure}

\subsubsection*{Diffusion Models}

Rombach et al. newly proposed Latent Diffusion Models as the state-of-art image generation algorithm \cite{Rombach2022High}.
The idea of diffusion is to learn an image style by denoising the training image to learn the data distribution in the image.
That is, gradually break down an image into noises.
Then for generation, they use the inverse map of diffusion to resample on noise under known distribution.

$$
L_{L D M}:=\mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_\theta\left(z_t, t\right)\right\|_2^2\right]
$$

As StyleGAN models specialize in an image-to-image generation,
LDMs show surprising results in text-to-image generation.
\begin{figure}[h]
    \includegraphics[width=\textwidth]{img/LDM-1.png}
    \caption{
        Latent Diffusion Models:
        user input text is on the top
        and the generated images are under the text descriptions \cite{Rombach2022High}.
    }
\end{figure}

When we apply LDM to anime characters,
we can easily generate any character we want with proper text description, as in Figure 4.
As shown in the image grid,
text-to-image generated images are like brand-new characters,
i.e., there is hard to find the exact same style character in existing anime as in StyleGAN generated images as in Figure 4.
\begin{figure}[]
    \begin{center}
    \includegraphics[width=\textwidth]{img/waifu_diffusion.png}
    \end{center}
    \caption{
        Waifu Diffusion:
        user input text is on the top
        and the generated images are under the text descriptions \cite{WaifuDiffusion}.
    }
\end{figure}

\newpage